{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3YdU0m95i3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377cca2f-fe77-4c3b-db47-039492d2aa44"
      },
      "source": [
        "%%writefile boston_house_price_prediction.py\n",
        "\n",
        "\n",
        "# Step -1 - Import Packages\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "\n",
        "\n",
        "\n",
        "# Step - 2 - Define the main function\n",
        "def main():\n",
        "    # Get data\n",
        "\n",
        "    ### To Do Assignment: try changing the data from Boston housing to California housing dataset\n",
        "    ### You can load the datasets as follows::\n",
        "    ###    from sklearn.datasets import fetch_california_housing\n",
        "    ###    housing = fetch_california_housing()\n",
        "    ###  Refer this link for more detatils: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
        "\n",
        "\n",
        "    # load the regression dataset\n",
        "    # Load the California housing dataset\n",
        "    from sklearn.datasets import fetch_california_housing\n",
        "    california_data = fetch_california_housing()\n",
        "\n",
        "    # Separate the features and target variable\n",
        "    california_X =pd.DataFrame(california_data.data, columns=california_data.feature_names)\n",
        "    california_y = california_data.target\n",
        "    features = california_data.feature_names\n",
        "\n",
        "    # check the data has loaded successfully\n",
        "    print(california_X.shape)\n",
        "    print(california_y.shape)\n",
        "\n",
        "    ## Data Exploration\n",
        "    print(f'The features in dataset are: {features}')\n",
        "    #print(f'Data description\\n {california_X.describe()}')\n",
        "\n",
        "    #Plots\n",
        "    plot_data(california_X, california_y, features, cor=True)\n",
        "\n",
        "    ## Remove Outliers\n",
        "    california_X, california_y = remove_outliers(california_X,california_y, features)\n",
        "\n",
        "    X_train, y_train, X_test, y_test = preprocess(california_X, california_y, features)\n",
        "\n",
        "    model = SVR()\n",
        "    model = train(model, X_train, y_train)\n",
        "    evaluate(model, X_test, y_test, bl= True)\n",
        "\n",
        "\n",
        "    # Get the best params using the optimizer based on SVR\n",
        "    # best_params = optimize_models(X_train, y_train)\n",
        "    #print(best_params)\n",
        "    ## Build Best Model\n",
        "    ##best_C= best_params['C']\n",
        "    #best_kernel = best_params['kernel']\n",
        "    #best_model = SVR(kernel = best_kernel, C= best_C)\n",
        "    #best_model = train(best_model, X_train, y_train)\n",
        "    #evaluate (best_model, X_test, y_test)\n",
        "\n",
        "    ### To Do Assignment Change the model to MLP  and accordingly change Grid search params\n",
        "    #Get the best params using the optimizer based on MLP regressor model from Grid Search\n",
        "\n",
        "    best_params = optimize_models(X_train, y_train)\n",
        "    print(best_params)\n",
        "    best_params.fit(X_train, y_train)\n",
        "    best_model = best_params.best_estimator_\n",
        "    # Retrain the best model with optimal hyperparameters\n",
        "    best_model = train(best_model, X_train, y_train)\n",
        "\n",
        "    evaluate (best_model, X_test, y_test)\n",
        "\n",
        "\n",
        "# Step - 3 - Plot graphs to understand data\n",
        "def plot_data(x_df, y_df,features, cor=False):\n",
        "    X = x_df.values\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(\"Price Distribution\")\n",
        "    plt.hist(y_df, bins=30)\n",
        "    plt.show()\n",
        "    #cols = x_df.columns()\n",
        "    fig, ax = plt.subplots(1, len(features), sharey=True, figsize=(20,5))\n",
        "    plt.title(\"Relationship between different input features and price\")\n",
        "    ax = ax.flatten()\n",
        "    for i, col in enumerate(features):\n",
        "        x = X[:,i]\n",
        "        y = y_df\n",
        "        ax[i].scatter(x, y, marker='o')\n",
        "        ax[i].set_title(col)\n",
        "        ax[i].set_xlabel(col)\n",
        "        ax[i].set_ylabel('MEDV')\n",
        "    plt.show()\n",
        "\n",
        "    if cor:\n",
        "      ### To Do Add the code to find and display correlation among\n",
        "      ### different features\n",
        "\n",
        "      # Create a DataFrame using the preprocessed features\n",
        "      df = pd.DataFrame(x_df, columns=features)\n",
        "      df['Target'] = y\n",
        "\n",
        "      # Calculate the correlation matrix\n",
        "      correlation_matrix = df.corr()\n",
        "\n",
        "      # Sort the correlation values with the target variable in descending order\n",
        "      correlation_with_target = correlation_matrix['Target'].abs().sort_values(ascending=False)\n",
        "\n",
        "      k = len(features)\n",
        "      selected_features = correlation_with_target[1:k+1].index.tolist()\n",
        "\n",
        "      print(\"Features in descending order of correlation:\", selected_features)\n",
        "\n",
        "\n",
        "# Step - 4 - Preprocess data\n",
        "# Step -4a : Remove outliers\n",
        "def remove_outliers(x,y, features):\n",
        "    #remove null\n",
        "    x_df = x.copy(deep=True)\n",
        "    x_df['MEDV'] = y\n",
        "    x_df.dropna(inplace=True)\n",
        "    return x_df[features], x_df['MEDV']\n",
        "\n",
        "\n",
        "# Step -4b : Normalize data\n",
        "def scale_numeric(df):\n",
        "    x = df.values\n",
        "    #scaler = preprocessing.StandardScaler()\n",
        "    ### To Do Assignment instead of StandardScaler use MinMaxScaler,\n",
        "    ### Also observe if scaling influences the results\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Step -4b : Preprocess data\n",
        "def preprocess(x, y, features):\n",
        "    x_df = x[features].copy(deep=True)\n",
        "    x_df = scale_numeric(x_df)\n",
        "    #print(len(x_df),len(y))\n",
        "    # Split data into train, test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x_df,y, test_size=0.3, random_state=1)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "# Step - 5 - train model\n",
        "def train(model,X_train, y_train):\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Step - 6 - Evaluate Model\n",
        "def evaluate(model, X_test, y_test, plot = True, print_results=True, bl=False):\n",
        "    y_pred = model.predict(X_test)\n",
        "    if print_results:\n",
        "      if bl:\n",
        "        print('\\n\\nBaseline Model Performance on Test Dataset:\\n')\n",
        "      else:\n",
        "        print('\\n\\nBest Model Performance on Test Dataset:\\n')\n",
        "      print('R^2:',metrics.r2_score(y_test, y_pred))\n",
        "      print('MAE:',metrics.mean_absolute_error(y_test, y_pred))\n",
        "      print('MSE:',metrics.mean_squared_error(y_test, y_pred))\n",
        "      print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "    if plot:\n",
        "      plt.scatter(y_test, y_pred)\n",
        "      plt.xlabel(\"Prices\")\n",
        "      plt.ylabel(\"Predicted prices\")\n",
        "      plt.title(\"Prices vs Predicted prices\")\n",
        "      plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step - 7 - Improve Model\n",
        "def optimize_models(X_train, y_train):\n",
        "\n",
        "  #params = {'kernel':['linear', 'rbf'], 'C':[1, 10]}\n",
        "  #model = SVR()\n",
        "  #clf = GridSearchCV(model, params)\n",
        "\n",
        "   ### To Do Assignment Change the model to MLP  and accordingly change Grid search params\n",
        "   param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'lbfgs'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "   }\n",
        "\n",
        "   # Create the MLP regressor model\n",
        "   # Got the error - ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
        "   # Raised max_iterations from to 1000,2000,3000 , used different data scalers, but no success\n",
        "   model = MLPRegressor(random_state=42, max_iter=3000)\n",
        "\n",
        "   # Perform Grid Search to find the best hyperparameters\n",
        "   grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "   return (grid_search)\n",
        "\n",
        "\n",
        "\n",
        "# call the main finction\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting boston_house_price_prediction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqCek9Qv5ojY"
      },
      "source": [
        "%run boston_house_price_prediction.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6CEot9f_zE_"
      },
      "source": [
        "%%writefile cancer_detection.py\n",
        "\n",
        "\n",
        "# Step -1 - Import Package\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "\n",
        "\n",
        "\n",
        "# Step - 2 - Define the main function\n",
        "def main():\n",
        "    # Get data\n",
        "    cancer_data = load_breast_cancer()\n",
        "    cancer_data_X = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)\n",
        "    cancer_data_y = cancer_data.target\n",
        "    features = cancer_data.feature_names\n",
        "\n",
        "    vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness']\n",
        "    ## Data Exploration\n",
        "    print(f'The features in dataset are: {features}')\n",
        "    #print(f'Data description\\n {cancer_data_X.describe()}')\n",
        "\n",
        "    #Plots\n",
        "    plot_data(cancer_data_X, cancer_data_y, features= vars, cor=True)\n",
        "\n",
        "    ## Remove Outliers\n",
        "    cancer_data_X, cancer_data_y = remove_outliers(cancer_data_X,cancer_data_y, features)\n",
        "\n",
        "    X_train, y_train, X_test, y_test = preprocess(cancer_data_X, cancer_data_y, features)\n",
        "\n",
        "    model = SVC(random_state=6)\n",
        "\n",
        "    model = train(model, X_train, y_train)\n",
        "\n",
        "    baseline = evaluate(model, X_test, y_test, bl=True)\n",
        "\n",
        "    best_params = optimize_models(X_train, y_train)\n",
        "    print(best_params)\n",
        "\n",
        "    ## Build Best Model\n",
        "    best_C= best_params['C']\n",
        "    best_kernel = best_params['kernel']\n",
        "\n",
        "    best_model = SVC(kernel = best_kernel, C= best_C, random_state=6)\n",
        "    best_model = train(best_model, X_train, y_train)\n",
        "    evaluate (best_model, X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step - 3 - Plot graphs to understand data\n",
        "def plot_data(x_df, y_df,features, cor=False):\n",
        "    X = x_df.copy(deep=True)\n",
        "    X['class'] = y_df\n",
        "    sns.pairplot(X, hue = 'class', vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness'] )\n",
        "    plt.show()\n",
        "\n",
        "    if cor:\n",
        "      corr = X[features].corr()\n",
        "      plt.figure(figsize=(10,10))\n",
        "      sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step - 4 - Preprocess data\n",
        "# Step -4a : Remove outliers\n",
        "def remove_outliers(x,y, features):\n",
        "    #remove null\n",
        "    x_df = x.copy(deep=True)\n",
        "    x_df['class'] = y\n",
        "    x_df.dropna(inplace=True)\n",
        "    return x_df[features], x_df['class']\n",
        "\n",
        "\n",
        "# Step -4b : Normalize data\n",
        "def scale_numeric(df):\n",
        "    x = df.values\n",
        "    scaler = preprocessing.StandardScaler()\n",
        "    x_scaled = scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Step -4b : Preprocess data\n",
        "def preprocess(x, y, features):\n",
        "    x_df = x[features].copy(deep=True)\n",
        "    x_df = scale_numeric(x_df)\n",
        "    #print(len(x_df),len(y))\n",
        "    # Split data into train, test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x_df,y, test_size=0.3, random_state=45)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step - 5 - train model\n",
        "def train(model,X_train, y_train):\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Step - 6 - Evaluate Model\n",
        "def evaluate(model, X_test, y_test, plot = True, print_results=True, bl=False):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "    acc = metrics.accuracy_score(y_test, y_pred)\n",
        "    if print_results:\n",
        "      if bl:\n",
        "        print('\\n\\nBaseline Model Performance on Test Dataset:\\n')\n",
        "      else:\n",
        "        print('\\n\\nBest Model Performance on Test Dataset:\\n')\n",
        "      print('\\nConfusion Matrix:\\n',cm)\n",
        "      print(f'Accuracy: {acc*100}%')\n",
        "\n",
        "    if plot:\n",
        "      sns.heatmap(cm, annot= True)\n",
        "      plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step - 7 - Improve Model\n",
        "def optimize_models(X_train, y_train):\n",
        "  params = {'kernel':['rbf'], 'C':[1.0, 5.0, 10]}\n",
        "  model = SVC(random_state=5)\n",
        "  clf = GridSearchCV(model, params)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf.best_params_\n",
        "\n",
        "\n",
        "# call the main finction\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwUe0B6NEtb8"
      },
      "source": [
        "%run cancer_detection.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwrKte3d5GBa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}